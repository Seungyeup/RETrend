import os
import time
import socket
import logging
from pyspark.sql import SparkSession

logging.basicConfig(level=logging.DEBUG)
print("ENV SPARK_REMOTE:", os.environ.get("SPARK_REMOTE"))
print("ENV SPARK_LOCAL_REMOTE:", os.environ.get("SPARK_LOCAL_REMOTE"))

# 환경 정리
os.environ["HADOOP_USER_NAME"] = "sparkuser"
os.environ["HOME"] = "/tmp"
ivy_cache = "/tmp/.ivy2"
os.makedirs(f"{ivy_cache}/local", exist_ok=True)

spark = SparkSession.builder \
    .appName("SparkConnectGateway") \
    .master("local[*]") \
    .config("spark.sql.connect.enabled", "true") \
    .config("spark.connect.grpc.server.port", "15002") \
    .config("spark.connect.grpc.server.bindAddress", "0.0.0.0") \
    .config("spark.hadoop.hadoop.security.authentication", "simple") \
    .config("spark.hadoop.hadoop.security.authorization", "false") \
    .config("spark.jars.ivy", ivy_cache) \
    .config("spark.dynamicAllocation.enabled", "false") \
    .config("spark.sql.connect.grpc.log.level", "DEBUG") \
    .getOrCreate()

print("Created SparkSession, version:", spark.version)
print("Effective Spark plugins and conf:")
conf = spark.sparkContext.getConf()
print("spark.plugins =", conf.get("spark.plugins", ""))
print("spark.sql.connect.enabled =", conf.get("spark.sql.connect.enabled"))
print("spark.connect.grpc.server.port =", conf.get("spark.connect.grpc.server.port"))

# 기존 wait logic
def wait_for_port(host: str, port: int, timeout: int = 60):
    start = time.time()
    while time.time() - start < timeout:
        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
            try:
                s.settimeout(1)
                s.connect((host, port))
                return True
            except Exception:
                time.sleep(1)
    return False

print("Waiting for gRPC port to become ready...")
if wait_for_port("0.0.0.0", 15002, timeout=30) or wait_for_port("localhost", 15002, timeout=30):
    print(f"Spark Connect gateway is up. Spark version: {spark.version}")
else:
    print("Warning: gRPC port did not become ready within timeout.")

try:
    while True:
        time.sleep(60)
except KeyboardInterrupt:
    pass
finally:
    spark.stop()
