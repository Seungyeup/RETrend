apiVersion: v1
kind: ServiceAccount
metadata:
  name: spark-connect
  namespace: spark-cluster

---

# apiVersion: rbac.authorization.k8s.io/v1
# kind: Role
# metadata:
#   name: spark-connect-role
#   namespace: spark-cluster
# rules:
#   - apiGroups: [""]
#     resources: ["pods", "configmaps", "secrets", "serviceaccounts", "services", "persistentvolumeclaims", "persistentvolumes"]
#     verbs: ["get", "list", "watch", "create", "delete", "patch", "update", "deletecollection"]
#   - apiGroups: ["batch", "apps"]
#     resources: ["jobs", "deployments", "replicasets"]
#     verbs: ["get", "list", "watch"]

apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: spark-connect-role
  namespace: spark-cluster
rules:
  - apiGroups: [""]
    resources: ["pods", "pods/log", "services", "configmaps", "secrets", "persistentvolumeclaims"]
    verbs: ["get", "list", "watch", "create", "delete", "patch", "update", "deletecollection"]
  - apiGroups: ["batch", "apps"]
    resources: ["jobs", "deployments", "replicasets"]
    verbs: ["get", "list", "watch"]

---

apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: spark-connect-binding
  namespace: spark-cluster
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: spark-connect-role
subjects:
  - kind: ServiceAccount
    name: spark-connect
    namespace: spark-cluster

---

apiVersion: apps/v1
kind: Deployment
metadata:
  name: spark-connect-gateway
  namespace: spark-cluster
  labels:
    app: spark-connect
spec:
  replicas: 1
  selector:
    matchLabels:
      app: spark-connect
  template:
    metadata:
      labels:
        app: spark-connect
    spec:
      serviceAccountName: spark-connect
      containers:
        - name: spark-connect
          image: dave126/spark-connect-gateway:1.2.8
          imagePullPolicy: Always
          env:
            - name: POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: HADOOP_USER_NAME
              value: "sparkuser"
            - name: HOME
              value: "/tmp"
            # - name: IVY_HOME
            #   value: "/home/spark/.ivy2"
          ports:
            - name: connect-grpc
              containerPort: 15002
            - name: driver-rpc
              containerPort: 15003
            - name: block-manager
              containerPort: 15004
          command: ["sh", "-c"]
          args:
            - >
              exec /opt/spark/bin/spark-submit
              --class org.apache.spark.sql.connect.service.SparkConnectServer
              --name spark-connect-gateway
              --master k8s://https://kubernetes.default.svc:443
              --conf spark.eventLog.enabled=true
              --conf spark.eventLog.dir=s3a://k8s-logs/
              --conf spark.hadoop.fs.s3a.endpoint=172.30.1.28:9000
              --conf spark.hadoop.fs.s3a.access.key=minioadmin
              --conf spark.hadoop.fs.s3a.secret.key=minioadmin
              --conf spark.hadoop.fs.s3a.path.style.access=true
              --conf spark.hadoop.fs.s3a.connection.ssl.enabled=false
              --conf spark.hadoop.fs.s3a.metadatastore.impl=org.apache.hadoop.fs.s3a.s3guard.NullMetadataStore
              --conf spark.hadoop.fs.s3a.authoritative=false
              --conf spark.kubernetes.namespace=spark-cluster
              --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark-connect
              --conf spark.kubernetes.container.image=dave126/spark-connect-gateway:1.2.8
              --conf spark.sql.connect.grpc.log.level=INFO
              --conf spark.connect.grpc.server.bindAddress=0.0.0.0
              --conf spark.connect.grpc.server.port=15002
              --conf spark.driver.bindAddress=0.0.0.0
              --conf spark.driver.host=spark-connect
              --conf spark.driver.host=$POD_IP
              --conf spark.driver.port=15003
              --conf spark.blockManager.bindAddress=0.0.0.0
              --conf spark.blockManager.host=spark-connect
              --conf spark.blockManager.host=$POD_IP
              --conf spark.blockManager.port=15004
              --conf spark.hadoop.hadoop.security.authentication=simple
              --conf spark.hadoop.hadoop.security.authorization=false
              --conf spark.local.dir=/tmp/spark-local
              --conf spark.sql.connect.artifact.rootDirectory=/tmp/connect-artifacts
              --conf spark.dynamicAllocation.enabled=true
              --conf spark.dynamicAllocation.minExecutors=1
              --conf spark.dynamicAllocation.maxExecutors=3
          volumeMounts:
            - name: connect-artifacts
              mountPath: /tmp/connect-artifacts
            - name: spark-local
              mountPath: /tmp/spark-local
            # - name: ivy-cache
            #   mountPath: /home/spark/.ivy2
              # mountPath: /tmp/.ivy
          readinessProbe:
            tcpSocket:
              port: 15002
            initialDelaySeconds: 15
            periodSeconds: 5
            timeoutSeconds: 2
            failureThreshold: 6
          livenessProbe:
            tcpSocket:
              port: 15002
            initialDelaySeconds: 60
            periodSeconds: 20
            timeoutSeconds: 5
            failureThreshold: 3
          resources:
            requests:
              cpu: "500m"
              memory: "1Gi"
            limits:
              cpu: "1"
              memory: "2Gi"
      volumes:
        - name: connect-artifacts
          emptyDir: {}
        - name: spark-local
          emptyDir: {}
        # - name: ivy-cache
        #   emptyDir: {}

---

apiVersion: v1
kind: Service
metadata:
  name: spark-connect
  namespace: spark-cluster
  labels:
    app: spark-connect
spec:
  selector:
    app: spark-connect
  ports:
    - name: connect
      port: 15002
      targetPort: 15002
    - name: driver-rpc
      port: 15003
      targetPort: 15003
    - name: block-manager
      port: 15004
      targetPort: 15004
  type: ClusterIP